<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Siri-Like AI Assistant</title>
  <style>
    body {
      margin: 0;
      overflow: hidden;
      background: linear-gradient(120deg, #1abc9c, #16a085);
      animation: gradientShift 8s infinite;
      font-family: Arial, sans-serif;
    }
    canvas {
      display: block;
    }
    .message, .response {
      position: absolute;
      left: 50%;
      transform: translateX(-50%);
      padding: 10px 20px;
      background: rgba(0, 0, 0, 0.7);
      color: #ffffff;
      font-size: 14px;
      border-radius: 5px;
      display: none;
    }
    .message {
      bottom: 20px;
    }
    .response {
      top: 20px;
      color: #00ffcc;
    }
    @keyframes gradientShift {
      0% { background: linear-gradient(120deg, #1abc9c, #16a085); }
      50% { background: linear-gradient(120deg, #1abc9c, #2ecc71); }
      100% { background: linear-gradient(120deg, #1abc9c, #16a085); }
    }
  </style>
</head>
<body>
  <div id="message" class="message">Click to start recording...</div>
  <div id="response" class="response"></div>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/0.153.0/three.min.js"></script>
  <script>
    let isRecording = false; // 录音状态
    let audioChunks = []; // 存储录音数据
    let audioContext, scriptProcessor, mediaStreamSource; // Web Audio API

    // 初始化 Three.js 场景
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
    const renderer = new THREE.WebGLRenderer({ antialias: true });
    renderer.setSize(window.innerWidth, window.innerHeight);
    document.body.appendChild(renderer.domElement);

    let smoothedVolume = 0; // 用于音量变化的平滑处理
    let speakingScale = 0; // 用于语音播放的动画

    const createSiriLikeEffect = () => {
      const group = new THREE.Group();

      // 外围光环
      const torusGeometry = new THREE.TorusGeometry(2, 0.1, 30, 100);
      const torusMaterial = new THREE.MeshBasicMaterial({
        color: 0x16a085,
        emissive: 0x1abc9c,
        emissiveIntensity: 0.8,
      });
      const torus = new THREE.Mesh(torusGeometry, torusMaterial);
      group.add(torus);

      // 中间 3D 球体
      const sphereGeometry = new THREE.SphereGeometry(0.5, 32, 32);
      const sphereMaterial = new THREE.MeshBasicMaterial({
        color: 0xffffff,
        emissive: 0x1abc9c,
        emissiveIntensity: 1.2,
      });
      const sphere = new THREE.Mesh(sphereGeometry, sphereMaterial);
      group.add(sphere);

      return { group, sphere };
    };

    const { group: siriEffect, sphere } = createSiriLikeEffect();
    scene.add(siriEffect);
    camera.position.z = 10;

    const animate = () => {
      siriEffect.rotation.y += 0.01; // 光环旋转
      siriEffect.rotation.x += 0.005; // 光环轻微倾斜

      // 中间球体动画：音量 + 语音播放双重控制
      const scale = 1 + smoothedVolume * 0.5 + speakingScale * 0.5; // 平滑变化
      sphere.scale.set(scale, scale, scale);

      renderer.render(scene, camera);
      requestAnimationFrame(animate);
    };
    animate();

    const startRecording = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        mediaStreamSource = audioContext.createMediaStreamSource(stream);
        scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);

        scriptProcessor.onaudioprocess = (event) => {
          const inputData = event.inputBuffer.getChannelData(0); // 获取音频数据
          const buffer = new Float32Array(inputData);
          audioChunks.push(buffer);
          const maxVolume = Math.max(...buffer); // 计算音量
          smoothedVolume += (maxVolume - smoothedVolume) * 0.1; // 指数平滑
        };

        mediaStreamSource.connect(scriptProcessor);
        scriptProcessor.connect(audioContext.destination);

        showMessage("Recording... Click again to stop.");
        isRecording = true;
      } catch (error) {
        console.error("Audio processing error:", error);
        showMessage("Unable to access microphone. Please check permissions.");
      }
    };

    const stopRecording = async () => {
      if (!isRecording) return;

      scriptProcessor.disconnect();
      mediaStreamSource.disconnect();
      audioContext.close();

      // 将 Float32Array 转换为 WAV 格式
      const wavBlob = createWavBlob(audioChunks, audioContext.sampleRate);

      const formData = new FormData();
      formData.append('audio', wavBlob, 'audio.wav');

      try {
        const response = await fetch('http://127.0.0.1:5000/api/audio', {
          method: 'POST',
          body: formData,
        });

        if (response.ok) {
          const result = await response.json();
          showResponse(result.message); // 显示后端返回内容
          playResponse(result.message); // 使用语音播放 AI 回复
        } else {
          showResponse("Error from server: " + response.statusText);
        }
      } catch (error) {
        console.error("Upload error:", error);
        showResponse("Failed to upload audio.");
      }

      isRecording = false;
    };

    /**
     * 将音频数据转换为 WAV 格式
     * @param {Float32Array[]} audioData
     * @param {number} sampleRate
     * @returns {Blob} WAV 格式的音频 Blob
     */
    const createWavBlob = (audioData, sampleRate) => {
      const buffer = mergeBuffers(audioData);
      const wav = encodeWav(buffer, sampleRate);
      return new Blob([wav], { type: 'audio/wav' });
    };

    const mergeBuffers = (buffers) => {
      const length = buffers.reduce((sum, buffer) => sum + buffer.length, 0);
      const result = new Float32Array(length);
      let offset = 0;
      buffers.forEach((buffer) => {
        result.set(buffer, offset);
        offset += buffer.length;
      });
      return result;
    };

    const encodeWav = (samples, sampleRate) => {
      const buffer = new ArrayBuffer(44 + samples.length * 2);
      const view = new DataView(buffer);

      const writeString = (offset, string) => {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      };

      writeString(0, 'RIFF');
      view.setUint32(4, 36 + samples.length * 2, true);
      writeString(8, 'WAVE');
      writeString(12, 'fmt ');
      view.setUint32(16, 16, true);
      view.setUint16(20, 1, true);
      view.setUint16(22, 1, true);
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, sampleRate * 2, true);
      view.setUint16(32, 2, true);
      view.setUint16(34, 16, true);
      writeString(36, 'data');
      view.setUint32(40, samples.length * 2, true);

      let offset = 44;
      for (let i = 0; i < samples.length; i++, offset += 2) {
        const s = Math.max(-1, Math.min(1, samples[i]));
        view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
      }

      return view;
    };

    const playResponse = (text) => {
      const utterance = new SpeechSynthesisUtterance(text);
      speechSynthesis.speak(utterance);
    };

    const showMessage = (msg) => {
      document.getElementById("message").textContent = msg;
    };

    const showResponse = (msg) => {
      const responseBox = document.getElementById("response");
      responseBox.textContent = msg;
      responseBox.style.display = "block";
      setTimeout(() => responseBox.style.display = "none", 5000);
    };

    window.addEventListener("click", () => {
      if (isRecording) stopRecording();
      else startRecording();
    });
  </script>
</body>
</html>
