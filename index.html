<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voicely Blockchain AI Assistant</title>
  <style>
    body {
      margin: 0;
      overflow: hidden;
      background: linear-gradient(120deg, #1abc9c, #16a085);
      animation: gradientShift 8s infinite;
      font-family: Arial, sans-serif;
    }
    canvas {
      display: block;
    }
    .message, .response {
      position: absolute;
      left: 50%;
      transform: translateX(-50%);
      padding: 10px 20px;
      background: rgba(0, 0, 0, 0.7);
      color: #ffffff;
      font-size: 14px;
      border-radius: 5px;
      display: none;
    }
    .message {
      bottom: 20px;
    }
    .response {
      top: 20px;
      color: #00ffcc;
    }
    @keyframes gradientShift {
      0% { background: linear-gradient(120deg, #1abc9c, #16a085); }
      50% { background: linear-gradient(120deg, #1abc9c, #2ecc71); }
      100% { background: linear-gradient(120deg, #1abc9c, #16a085); }
    }
  </style>
</head>
<body>
  <div id="message" class="message">Click to start recording...</div>
  <div id="response" class="response"></div>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/0.153.0/three.min.js"></script>
  <script>
    let isRecording = false; // 录音状态
    let audioChunks = []; // 用于存储录音的音频数据
    let audioContext, mediaRecorder, processor, input;

    // 初始化 Three.js 场景
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
    const renderer = new THREE.WebGLRenderer({ antialias: true });
    renderer.setSize(window.innerWidth, window.innerHeight);
    document.body.appendChild(renderer.domElement);

    let smoothedVolume = 0; // 用于音量变化的平滑处理
    let speakingScale = 0; // 用于语音播放的动画

    const createSiriLikeEffect = () => {
      const group = new THREE.Group();

      // 外围光环
      const torusGeometry = new THREE.TorusGeometry(2, 0.1, 30, 100);
      const torusMaterial = new THREE.MeshBasicMaterial({
        color: 0x16a085,
        emissive: 0x1abc9c,
        emissiveIntensity: 0.8,
      });
      const torus = new THREE.Mesh(torusGeometry, torusMaterial);
      group.add(torus);

      // 中间 3D 球体
      const sphereGeometry = new THREE.SphereGeometry(0.5, 32, 32);
      const sphereMaterial = new THREE.MeshBasicMaterial({
        color: 0xffffff,
        emissive: 0x1abc9c,
        emissiveIntensity: 1.2,
      });
      const sphere = new THREE.Mesh(sphereGeometry, sphereMaterial);
      group.add(sphere);

      return { group, sphere };
    };

    const { group: siriEffect, sphere } = createSiriLikeEffect();
    scene.add(siriEffect);
    camera.position.z = 10;

    const animate = () => {
      siriEffect.rotation.y += 0.01; // 光环旋转
      siriEffect.rotation.x += 0.005; // 光环轻微倾斜

      // 中间球体动画：音量 + 语音播放双重控制
      const scale = 1 + smoothedVolume * 0.5 + speakingScale * 0.5; // 平滑变化
      sphere.scale.set(scale, scale, scale);

      renderer.render(scene, camera);
      requestAnimationFrame(animate);
    };
    animate();

    const startRecording = async () => {
      try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        input = audioContext.createMediaStreamSource(stream);
        processor = audioContext.createScriptProcessor(2048, 1, 1);

        audioChunks = []; // 清空音频数据

        processor.onaudioprocess = (event) => {
          const inputBuffer = event.inputBuffer.getChannelData(0);
          audioChunks.push(new Float32Array(inputBuffer)); // 收集音频数据
        };

        input.connect(processor);
        processor.connect(audioContext.destination);

        showMessage("I am listening... Click again to stop.");
        isRecording = true;

        visualizeAudio(stream);
      } catch (error) {
        console.error("Audio processing error:", error);
        showMessage("Unable to access microphone. Please check permissions.");
      }
    };

    const stopRecording = () => {
      if (processor) processor.disconnect();
      if (input) input.disconnect();
      if (audioContext) audioContext.close();

      const wavBlob = encodeWAV(audioChunks);
      uploadAudio(wavBlob);

      showMessage("I'm thinking...");
      isRecording = false;
    };

    const encodeWAV = (samples) => {
      const buffer = mergeBuffers(samples);
      const wavData = writeWAV(buffer);
      return new Blob([wavData], { type: "audio/wav" });
    };

    const mergeBuffers = (buffers) => {
      const length = buffers.reduce((sum, buffer) => sum + buffer.length, 0);
      const result = new Float32Array(length);
      let offset = 0;
      for (const buffer of buffers) {
        result.set(buffer, offset);
        offset += buffer.length;
      }
      return result;
    };

    const writeWAV = (samples) => {
      const sampleRate = audioContext.sampleRate;
      const bufferLength = samples.length;
      const dataLength = bufferLength * 2;
      const wavBuffer = new ArrayBuffer(44 + dataLength);
      const view = new DataView(wavBuffer);

      // WAV header
      writeString(view, 0, "RIFF");
      view.setUint32(4, 36 + dataLength, true);
      writeString(view, 8, "WAVE");
      writeString(view, 12, "fmt ");
      view.setUint32(16, 16, true);
      view.setUint16(20, 1, true);
      view.setUint16(22, 1, true);
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, sampleRate * 2, true);
      view.setUint16(32, 2, true);
      view.setUint16(34, 16, true);
      writeString(view, 36, "data");
      view.setUint32(40, dataLength, true);

      // PCM samples
      let offset = 44;
      for (let i = 0; i < bufferLength; i++) {
        const sample = Math.max(-1, Math.min(1, samples[i]));
        view.setInt16(offset, sample * 0x7fff, true);
        offset += 2;
      }

      return view;
    };

    const writeString = (view, offset, string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    };

    const uploadAudio = async (blob) => {
      const formData = new FormData();
      formData.append("audio", blob, "audio.wav");

      try {
        const response = await fetch("http://127.0.0.1:5000/api/audio", {
          method: "POST",
          body: formData,
        });

        if (response.ok) {
          const result = await response.json();
          showResponse(result.message); // 显示后端返回内容
          playResponse(result.message); // 使用语音播放 AI 回复
        } else {
          showResponse("Error from server: " + response.statusText);
        }
      } catch (error) {
        console.error("Upload error:", error);
        showResponse("Internet error.");
      }
    };

    const visualizeAudio = (stream) => {
      const analyser = audioContext.createAnalyser();
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      input.connect(analyser);

      const visualize = () => {
        analyser.getByteFrequencyData(dataArray);
        const maxVolume = Math.max(...dataArray) / 255.0;
        smoothedVolume += (maxVolume - smoothedVolume) * 0.1; // 指数平滑
        if (isRecording) requestAnimationFrame(visualize);
      };
      visualize();
    };

    const playResponse = (text) => {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = "en-US";
      utterance.rate = 1;
      utterance.pitch = 1;

      utterance.onstart = () => (speakingScale = 1);
      utterance.onend = () => (speakingScale = 0);

      speechSynthesis.speak(utterance);
    };

    const showMessage = (msg) => {
      const messageBox = document.getElementById("message");
      messageBox.textContent = msg;
      messageBox.style.display = "block";
    };

    const showResponse = (msg) => {
      const responseBox = document.getElementById("response");
      responseBox.textContent = msg;
      responseBox.style.display = "block";
      setTimeout(() => {
        responseBox.style.display = "none";
      }, 5000);
    };

    window.addEventListener("click", () => {
      if (isRecording) stopRecording();
      else startRecording();
    });
  </script>
</body>
</html>
